---
title: "Formstack Page Analysis - Proposed Methodology"
output: html_notebook
date: "November 23, 2016"
---

The following will constitute a proposed methodology for the analysis of   data as part of the on-going mass.gov redesign. An understanding of top performing pages and departments, as well as under performing pages and departments, will be critical as content is migrated from the old percussion site to the new drupal site. 

# The Data 

Formstack is a service which allows websites to collect survey responses from users. MassIT uses Formstack to collect simple information from mass.gov visitors concerning their ability to find the information or service they are looking for on a given webpage. The survey typically looks like so:

<br>
<center>
<div style="width:450px; height=250px">
![](/Users/Connor/Desktop/formstack.png)
</div> 
</center>
<br>

If a user selects "No" then they are presented with the following:

<br>

<center><div style="width:450px; height=600px">
![](/Users/Connor/Desktop/formstack_no.png)
</div>
</center>

<br>
  
These survey responses appear to be a rich source of data regarding the findability of information and services on mass.gov, as well as the overall site's health and user satisfaction.

# Methodology

This section will lay out the methodology for the analysis of Formstack data which will occur daily for all of mass.gov. This analysis will be fed into a dashboard aimed at giving decision makers insights into site performance, funnel performance, 

## Creating Base Data 
First a master data set is created which will contain all responses for all organizations who contribute data. 

The following is the "head" of the formstack data set, which was created by merging all of the individual "Feedback" forms into a single table, removing any personal information, removing ip ranges which are known to be associated with the Commonwealth, and renaming of columns to machine friendly strings: 
\n
```{r}
library(magrittr)
library(ggplot2)

new.cols <- c("submit_time", "info_found", "problem_desc", "site", "content_author",
              "child_content_author", "referrer", "ip_addr", "id", "long", "lat", 
              "browser", "os")

formstack.master <- readr::read_csv('/Users/Connor/Documents/GitHub/GoogleAnalyticsSegmentation/code/dashboard/src/data/formstack/formstack_master.csv') %>%
  dplyr::rename_(.dots = setNames(object = paste0("`", names(.), "`"), nm = new.cols)) %>%
  purrr::map_if(is.character, stringr::str_trim) %>%
  data.frame() %>%
  dplyr::filter(ip_addr != "^(146\\.243\\.\\d{1,3}|170\\.63\\.\\d{1,3}|170\\.154\\.\\d{1,3}|65\\.217\\.255\\.\\d{1,3}|4.36.198.102|65.118.148.102|204.166.193.130|204.130.104.10)",
                info_found %in% c("Yes", "No")) %>%
  mutate(info_found = droplevels(info_found))
```

```{r}
head(formstack.master)
```

## Sitewide improvements
The key question Formstack data will assist with is "are the changes made to the new mass.gov an improvement relative to the old site?" This question is answerable with the proposed framework and data, given a few other conditions are met. First, a consistent mapping between old and new content must be created, this will done in the form of page redirects. Second, the feedback module should remain as consistent as possible in the switch between old and new because if, for example, a change in position would increase the average number of people answering in the affirmative we would not want to mix up the effects of that repositioning and the new content itself. There is a situation where point 2 is not as much of an issue, which is getting someone at the leadership level (preferrably Harlan) to estimate a prior distribution for the response rate and modelling the feedback as bayesian inference. 




### Bayesian Inference

In any type of binomial inference we are concerned with estimating some probability that a random variable takes one of two possible values $p(Y)$, in this case an affirmative response. Clearly, this is a system best modelled by a binomial distribution meaning:

$$ 
p(Y) = \binom{N}{Y}\theta^Y(1-\theta)^{(N-Y)}
$$
Where $N$ is the number of bernoulli trials, $Y$ is the random variable representing the outcome of interest, and $\theta$ represents the unkown value bounded by 0 and 1 the equals the true proportion $Y/N$. In english, this means that the distribution models the probability that a person affirms finding the desired content $Y$ times in $N$ trials. Skipping some steps in the interest of brevity we can arrive at the following in a straightforward manner:

$$
p(\theta_{affirm}|Y_{samp},N_{samp}) \propto p(\theta_{affirm})p(Y_{samp}, N_{samp}|\theta_{affirm})
$$
meaning **our derived posterior distribution is proportional to the product of our prior and likelihood!!** More generally this means that our inputs AND our outputs are all distributions. We also have the fortune of leveraging conjugacy which means that when our likelihood and prior are from the same family ([binomial likelihood and beta prior](https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions)) we get the same kind of distribution out that we put in! 

In order to begin any bayesian inference for binomially distributed data we must specify a prior. This should be done in one of three ways 1) in conjunction with a subject matter expert who can provide an expected $\theta$ and some feedback on the range of values which are likely 2) using what is called an "uninformative prior" which states that all outcomes are equally likely (in the case of a beta distribution $B(\alpha, \beta)$ this takes the values of $\alpha = 1$ $\beta = 1$) or 3) fit an empirical likelihood modelled on data available. 

```{r}
betaplot <- function(a,b){
theta = seq(0,1,0.005)
p_theta = dbeta(theta, a, b)
p <- qplot(theta, p_theta, geom='line')
p <- p + theme_bw()
p <- p + ylab(expression(paste('p(',theta,')', sep = '')))
p <- p + xlab(expression(theta))
return(p)}
```

```{r}
betaplot(1,1)
```

This is an extremely flexible framework that works well with small data where initial summarization may paint a misleading picture. For example, estimating a baseball players batting average is straightforward for someone who has played for many years but for a new player a straight average may paint a very misleading picture. In this example it is more sound to integrate our knowledge of the expected value and distribution of a metric instead of forging ahead as normal and reporting something like a 1.0 or 0.0 as a players average. 


#### Further Examples 
Given the probability mass function above it is therefore evident that:

$$
p(y_i) = \theta^{y_i}(1-\theta)^{(1-{y_i})}
$$
and 

$$
p(Y)=\prod^N_{i=1}\theta^{y_i}(1-\theta)^{(1-y_{i})}
$$
because the probability of a number of independent events occuring together is the product of all of their individual probabilities!














